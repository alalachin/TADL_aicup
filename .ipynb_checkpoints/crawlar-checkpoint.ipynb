{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get links from a file and extract the links into a list \"links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "links = pd.read_csv(\"NC_1.csv\")\n",
    "\n",
    "index = list(links[\"Index\"])\n",
    "links = list(links[\"URL\"])\n",
    "\n",
    "# run the following code only once!\n",
    "for i in range(len(link)):\n",
    "    if (links[i].find(\"chinatimes\") != -1):\n",
    "        links[i] = links[i] + \"?chdtv\"\n",
    "    # print (links[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare an empty list \"dataset\" which is about to hold all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyquery import PyQuery as pq\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules for different news channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvbs_crawlar(link):\n",
    "    res = requests.get(link)\n",
    "    doc = pq(res.text)\n",
    "    tup = []\n",
    "    tup.append(doc(\"body > div.container > div.master > div.main > div.content > div.padding20_mo > div.content_left2 > div:nth-child(2) > div.newsdetail_content > div.title.margin_b20 > h1\").text())\n",
    "    tup.append(doc(\"#news_detail_div\").text())\n",
    "    return tup\n",
    "\n",
    "\n",
    "def apple_crawlar(link):\n",
    "    res = requests.get(link)\n",
    "    doc = pq(res.text)\n",
    "    tup = []\n",
    "    tup.append(doc(\"#article > div.wrapper > div > main > article > hgroup > h1\").text())\n",
    "    tup.append(doc(\"#article > div.wrapper > div > main > article > div.thoracis > div.ndArticle_contentBox > article > div.ndArticle_margin > p:nth-child(n)\").text())\n",
    "    if tup[1]=='':\n",
    "        tup[1]=doc(\"#article > div.wrapper > div > main > article > div.ndArticle_contentBox > article > div.ndArticle_margin > p:nth-child(n)\").text()\n",
    "    return tup\n",
    "\n",
    "\n",
    "def china_crawlar(link):\n",
    "    res = requests.get(link)\n",
    "    doc = pq(res.text)\n",
    "    tup = []\n",
    "    tup.append(doc(\"#page-top > div > div:nth-child(2) > div > div > article > div > header > h1\").text())\n",
    "    tup.append(doc(\"#page-top > div > div:nth-child(2) > div > div > article > div > div:nth-child(n) > div.row > div.col-xl-11 > div.article-body\").text())\n",
    "    return tup\n",
    "\n",
    "\n",
    "def ltn_crawlar(link):\n",
    "    res = requests.get(link)\n",
    "    doc = pq(res.text)\n",
    "    tup = []\n",
    "    tup.append(doc(\"body > div.content > section > div.whitecon.articlebody > h1\").text())\n",
    "    tup.append(doc(\"body > div.content > section > div.whitecon.articlebody > div.text > p:nth-child(n)\").text())\n",
    "    return tup\n",
    "\n",
    "\n",
    "def udn_crawlar(link):\n",
    "    res = requests.get(link)\n",
    "    doc = pq(res.text)\n",
    "    tup = []\n",
    "    tup.append(doc(\"#story_art_title\").text())\n",
    "    tup.append(doc(\"#story_body_content > p:nth-child(n)\").text())\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl news ( [title, content] ),  and store them into a big list \"dataset\"\n",
    "dataset structure: [news_index, [title, content], news_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(links)):\n",
    "    \n",
    "    if (links[i].find(\"chinatimes\") != -1):\n",
    "        tup = china_crawlar(links[i])   \n",
    "                \n",
    "    elif (links[i].find(\"appledaily\") != -1 and links[i].find(\"http\") != -1):\n",
    "        tup = apple_crawlar(links[i])        \n",
    "        \n",
    "    elif (links[i].find(\".ltn.\") != -1):\n",
    "        tup = ltn_crawlar(links[i])\n",
    "        \n",
    "    elif (links[i].find(\"udn.\") != -1):\n",
    "        tup = udn_crawlar(links[i])\n",
    "        \n",
    "    elif (links[i].find(\".tvbs.\") != -1):\n",
    "        tup = tvbs_crawlar(links[i])\n",
    "        \n",
    "    element = []    \n",
    "    element.append(index[i])\n",
    "    element.append(tup)\n",
    "    element.append(links[i])\n",
    "    dataset.append(element)\n",
    "    # print (element[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the crawled news to another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('miss2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Index', 'Text', 'URL'])\n",
    "    for ele in dataset:\n",
    "        writer.writerow(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the news without both title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_t&c2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Index', 'Link'])\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][1]==['','']:\n",
    "            temp = []\n",
    "            temp.append(dataset[i][0])\n",
    "            temp.append(dataset[i][2])\n",
    "            writer.writerow(temp)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the news with title but not content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_c.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Index', 'Link'])\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][1][1]=='' and dataset[i][1][0]!='':\n",
    "            temp = []\n",
    "            temp.append(dataset[i][0])\n",
    "            temp.append(dataset[i][2])\n",
    "            writer.writerow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
